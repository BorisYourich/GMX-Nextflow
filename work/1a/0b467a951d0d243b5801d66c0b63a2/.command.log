8
:-) GROMACS - gmx_mpi, 2022.2 (-: Executable: /gromacs/bin.AVX2_256/gmx_mpi Data prefix: /gromacs Working dir: /home/boris/GMX-Nextflow/work/1a/0b467a951d0d243b5801d66c0b63a2 Command line: gmx_mpi --version GROMACS version: 2022.2 Precision: mixed Memory model: 64 bit MPI library: MPI (CUDA-aware) OpenMP support: enabled (GMX_OPENMP_MAX_THREADS = 128) GPU support: CUDA SIMD instructions: AVX2_256 CPU FFT library: fftw-3.3.8-sse2-avx-avx2-avx2_128 GPU FFT library: cuFFT RDTSCP usage: disabled TNG support: enabled Hwloc support: disabled Tracing support: disabled C compiler: /usr/local/openmpi/bin/mpicc GNU 8.4.0 C compiler flags: -mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -O3 -DNDEBUG C++ compiler: /usr/local/openmpi/bin/mpicxx GNU 8.4.0 C++ compiler flags: -mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA compiler: /usr/local/cuda/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver;Copyright (c) 2005-2022 NVIDIA Corporation;Built on Tue_Mar__8_18:18:20_PST_2022;Cuda compilation tools, release 11.6, V11.6.124;Build cuda_11.6.r11.6/compiler.31057947_0 CUDA compiler flags:-std=c++17;;-D_FORCE_INLINES;-mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA driver: 11.60 CUDA runtime: N/A
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 15
slots that were requested by the application:

  gmx

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
