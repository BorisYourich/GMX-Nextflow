Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Reading file jovial_cantor.tpr, VERSION 2021-MODIFIED (single precision)
Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

This is simulation 0 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 1 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 2 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

This is simulation 3 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 4 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 7 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 8 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 9 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 6 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 5 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 10 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 11 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 12 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

This is simulation 13 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Using 1 MPI process
This is simulation 14 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 



WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.
WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.


WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.
WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.


NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads
NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.1#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.1#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.2#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.2#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.3#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.3#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.4#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.4#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.5#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.5#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.6#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.6#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.7#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.7#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.8#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.8#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.9#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.9#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.10#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.10#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.11#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.11#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.12#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.12#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.13#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.13#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_px.part0001.xvg.14#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/jovial_cantor_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#jovial_cantor_pf.part0001.xvg.14#
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    6 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    13 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 13
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    0 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    3 (out of 15)

Unknown exception:

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    2 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 2

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    9 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 9

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    11 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    12 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 12
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 6
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    4 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

application called MPI_Abort(MPI_COMM_WORLD, 1) - process 4
For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 3

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    14 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 14

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    1 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    5 (out of 15)

Unknown exception:

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    8 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found


+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 5
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 8

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    7 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 7

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
MPI rank:    10 (out of 15)

Unknown exception:
(exception type: N4PLMD14ExceptionErrorE)

+++ PLUMED error
+++ at IFile.cpp:113, function virtual PLMD::IFile& PLMD::IFile::open(const
string&)
+++ assertion failed: do_exist
+++ message follows +++
file jovial_cantor.dat cannot be found

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10
