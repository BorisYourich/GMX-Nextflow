8
:-) GROMACS - gmx, 2021-MODIFIED (-: GROMACS is written by: Andrey Alekseenko Emile Apol Rossen Apostolov Paul Bauer Herman J.C. Berendsen Par Bjelkmar Christian Blau Viacheslav Bolnykh Kevin Boyd Aldert van Buuren Rudi van Drunen Anton Feenstra Gilles Gouaillardet Alan Gray Gerrit Groenhof Anca Hamuraru Vincent Hindriksen M. Eric Irrgang Aleksei Iupinov Christoph Junghans Joe Jordan Dimitrios Karkoulis Peter Kasson Jiri Kraus Carsten Kutzner Per Larsson Justin A. Lemkul Viveca Lindahl Magnus Lundborg Erik Marklund Pascal Merz Pieter Meulenhoff Teemu Murtola Szilard Pall Sander Pronk Roland Schulz Michael Shirts Alexey Shvetsov Alfons Sijbers Peter Tieleman Jon Vincent Teemu Virolainen Christian Wennberg Maarten Wolf Artem Zhmurov and the project leaders: Mark Abraham, Berk Hess, Erik Lindahl, and David van der Spoel Copyright (c) 1991-2000, University of Groningen, The Netherlands. Copyright (c) 2001-2019, The GROMACS development team at Uppsala University, Stockholm University and the Royal Institute of Technology, Sweden. check out http://www.gromacs.org for more information. GROMACS is free software; you can redistribute it and/or modify it under the terms of the GNU Lesser General Public License as published by the Free Software Foundation; either version 2.1 of the License, or (at your option) any later version. GROMACS: gmx, version 2021-MODIFIED Executable: /gromacs/AVX2_256_ts/bin/gmx Data prefix: /gromacs/AVX2_256_ts Working dir: /home/boris/GMX-Nextflow/work/56/fc6f6526fb5e9b04039634ee83b1dc Command line: gmx --version GROMACS version: 2021-MODIFIED This program has been built from source code that has been altered and does not match the code released as part of the official GROMACS version 2021-MODIFIED. If you did not intend to use an altered GROMACS version, make sure to download an intact source distribution and compile that before proceeding. If you have modified the source code, you are strongly encouraged to set your custom version suffix (using -DGMX_VERSION_STRING_OF_FORK) which will can help later with scientific reproducibility but also when reporting bugs. Release checksum: 3e06a5865d6ff726fc417dea8d55afd37ac3cbb94c02c54c76d7a881c49c5dd8 Computed checksum: 08d4305cb3e76fffee7c24d458e792ae2a6857744772b5af11fd654d5decfb97 Precision: mixed Memory model: 64 bit MPI library: MPI OpenMP support: enabled (GMX_OPENMP_MAX_THREADS = 64) GPU support: CUDA SIMD instructions: AVX2_256 FFT library: fftw-3.3.8-sse2-avx-avx2-avx2_128 RDTSCP usage: enabled TNG support: enabled Hwloc support: disabled Tracing support: disabled C compiler: /usr/bin/gcc GNU 9.3.0 C compiler flags: -mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -O3 -DNDEBUG C++ compiler: /usr/bin/g++ GNU 9.3.0 C++ compiler flags: -mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA compiler: /usr/local/cuda/bin/nvcc nvcc: NVIDIA (R) Cuda compiler driver;Copyright (c) 2005-2021 NVIDIA Corporation;Built on Sun_Feb_14_21:12:58_PST_2021;Cuda compilation tools, release 11.2, V11.2.152;Build cuda_11.2.r11.2/compiler.29618528_0 CUDA compiler flags:-std=c++17;-gencode;arch=compute_35,code=sm_35;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-Wno-deprecated-gpu-targets;-gencode;arch=compute_35,code=compute_35;-gencode;arch=compute_50,code=compute_50;-gencode;arch=compute_52,code=compute_52;-gencode;arch=compute_60,code=compute_60;-gencode;arch=compute_61,code=compute_61;-gencode;arch=compute_70,code=compute_70;-gencode;arch=compute_75,code=compute_75;-gencode;arch=compute_80,code=compute_80;-use_fast_math;-D_FORCE_INLINES;-mavx2 -mfma -Wno-missing-field-initializers -fexcess-precision=fast -funroll-all-loops -fopenmp -O3 -DNDEBUG CUDA driver: 0.0 CUDA runtime: N/A
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Reading file special_yonath.tpr, VERSION 2021-MODIFIED (single precision)
Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.326

Changing nstlist from 20 to 100, rlist from 1.21 to 1.325

This is simulation 0 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:
This is simulation 8 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 10 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 9 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process

Using 1 MPI process
This is simulation 12 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 1 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 4 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 11 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 13 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 14 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 2 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 6 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 3 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 5 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
This is simulation 7 out of 15 running as a composite GROMACS
multi-simulation job. Setup for this simulation:

Using 1 MPI process
Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread Using 1 OpenMP thread 

Using 1 OpenMP thread 



Using 1 OpenMP thread 

Using 1 OpenMP thread Using 1 OpenMP thread 



Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 

Using 1 OpenMP thread 



WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.
WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.


WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.
WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

WARNING: On rank 0: oversubscribing the available 8 logical CPU cores per node with 15 MPI processes.
         This will cause considerable performance loss.

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads


NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads
NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

NOTE: Oversubscribing the CPU, will not pin threads

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.1#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.1#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.2#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.2#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.3#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.3#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.4#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.4#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.5#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.5#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.6#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.6#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.7#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.8#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.7#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.9#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.8#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.10#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.9#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.11#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.10#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.12#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.11#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.13#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_px.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_px.part0001.xvg.12#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_pf.part0001.xvg to /home/boris/GMX-Nextflow/RE/#special_yonath_pf.part0001.xvg.14#
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
starting mdrun 'Protein in water'
7000000 steps,  14000.0 ps.
step 0
Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.1#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.2#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.3#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.4#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.5#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.6#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.7#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.8#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.9#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.10#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.11#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.12#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.13#

Back Off! I just backed up /home/boris/GMX-Nextflow/RE/special_yonath_step100.cpt to /home/boris/GMX-Nextflow/RE/#special_yonath_step100.cpt.14#

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    5 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    11 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    4 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    7 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    14 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    6 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    10 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    8 (out of 15)

File input/output error:

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    1 (out of 15)
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    2 (out of 15)


-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    13 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    9 (out of 15)


-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    12 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------

-------------------------------------------------------
Program:     gmx mdrun, version 2021-MODIFIED
Source file: src/gromacs/mdlib/mdoutf.cpp (line 463)
MPI rank:    3 (out of 15)

File input/output error:
Cannot rename checkpoint file; maybe you are out of disk space?

For more information and tips for troubleshooting, please check the GROMACS
website at http://www.gromacs.org/Documentation/Errors
-------------------------------------------------------
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 1
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 5
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 6
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 14
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 4
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 7
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 9
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 10
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 3
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 8
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 12
application called MPI_Abort(MPI_COMM_WORLD, 1) - process 13
step 100, will finish Fri Feb  5 12:23:28 2027application called MPI_Abort(MPI_COMM_WORLD, 1) - process 2
